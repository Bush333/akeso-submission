{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOTE :\n",
    "\n",
    "### Kindly give the path to your datasets properly. I have adjusted based on the file structure on kaggle. Adjustment is needed only in last cell\n",
    "\n",
    "### Due to limited compute and storage resources, we have considered to process and work with only 3 subjects [p00,p01,p02].\n",
    "### For the leave-out strategy we have used only p00\n",
    "\n",
    "## \"TODO\" sections :  where we can add all 15 subject ids instead of 3, when good compute and storage units are available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-07-11T15:30:06.585251Z",
     "iopub.status.busy": "2024-07-11T15:30:06.584501Z",
     "iopub.status.idle": "2024-07-11T15:30:19.381064Z",
     "shell.execute_reply": "2024-07-11T15:30:19.380050Z",
     "shell.execute_reply.started": "2024-07-11T15:30:06.585204Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow==2.15.1 in /opt/conda/lib/python3.10/site-packages (2.15.1)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15.1) (1.4.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15.1) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15.1) (23.5.26)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15.1) (0.5.4)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15.1) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15.1) (3.10.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15.1) (16.0.6)\n",
      "Requirement already satisfied: ml-dtypes~=0.3.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15.1) (0.3.2)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15.1) (1.26.4)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15.1) (3.3.0)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15.1) (21.3)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15.1) (3.20.3)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15.1) (69.0.3)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15.1) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15.1) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15.1) (4.9.0)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15.1) (1.14.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15.1) (0.35.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15.1) (1.59.3)\n",
      "Requirement already satisfied: tensorboard<2.16,>=2.15 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15.1) (2.15.1)\n",
      "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15.1) (2.15.0)\n",
      "Requirement already satisfied: keras<2.16,>=2.15.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.15.1) (2.15.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow==2.15.1) (0.42.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.1) (2.26.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.1) (1.2.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.1) (3.5.2)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.1) (2.32.3)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.1) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.1) (3.0.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->tensorflow==2.15.1) (3.1.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.1) (4.2.4)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.1) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.1) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow==2.15.1) (1.3.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.1) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.1) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.1) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.1) (2024.2.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow==2.15.1) (2.1.3)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.1) (0.5.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow==2.15.1) (3.2.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow==2.15.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-11T15:30:25.030283Z",
     "iopub.status.busy": "2024-07-11T15:30:25.029915Z",
     "iopub.status.idle": "2024-07-11T15:30:45.427054Z",
     "shell.execute_reply": "2024-07-11T15:30:45.425935Z",
     "shell.execute_reply.started": "2024-07-11T15:30:25.030253Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tf-models-official==2.15\n",
      "  Downloading tf_models_official-2.15.0-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: Cython in /opt/conda/lib/python3.10/site-packages (from tf-models-official==2.15) (3.0.8)\n",
      "Requirement already satisfied: Pillow in /opt/conda/lib/python3.10/site-packages (from tf-models-official==2.15) (9.5.0)\n",
      "Collecting gin-config (from tf-models-official==2.15)\n",
      "  Downloading gin_config-0.5.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: google-api-python-client>=1.6.7 in /opt/conda/lib/python3.10/site-packages (from tf-models-official==2.15) (2.131.0)\n",
      "Collecting immutabledict (from tf-models-official==2.15)\n",
      "  Downloading immutabledict-4.2.0-py3-none-any.whl.metadata (3.4 kB)\n",
      "Requirement already satisfied: kaggle>=1.3.9 in /opt/conda/lib/python3.10/site-packages (from tf-models-official==2.15) (1.6.14)\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (from tf-models-official==2.15) (3.7.5)\n",
      "Requirement already satisfied: numpy>=1.20 in /opt/conda/lib/python3.10/site-packages (from tf-models-official==2.15) (1.26.4)\n",
      "Requirement already satisfied: oauth2client in /opt/conda/lib/python3.10/site-packages (from tf-models-official==2.15) (4.1.3)\n",
      "Requirement already satisfied: opencv-python-headless in /opt/conda/lib/python3.10/site-packages (from tf-models-official==2.15) (4.10.0.82)\n",
      "Requirement already satisfied: pandas>=0.22.0 in /opt/conda/lib/python3.10/site-packages (from tf-models-official==2.15) (2.2.1)\n",
      "Requirement already satisfied: psutil>=5.4.3 in /opt/conda/lib/python3.10/site-packages (from tf-models-official==2.15) (5.9.3)\n",
      "Requirement already satisfied: py-cpuinfo>=3.3.0 in /opt/conda/lib/python3.10/site-packages (from tf-models-official==2.15) (9.0.0)\n",
      "Collecting pycocotools (from tf-models-official==2.15)\n",
      "  Downloading pycocotools-2.0.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: pyyaml>=6.0.0 in /opt/conda/lib/python3.10/site-packages (from tf-models-official==2.15) (6.0.1)\n",
      "Collecting sacrebleu (from tf-models-official==2.15)\n",
      "  Downloading sacrebleu-2.4.2-py3-none-any.whl.metadata (58 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.0/58.0 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: scipy>=0.19.1 in /opt/conda/lib/python3.10/site-packages (from tf-models-official==2.15) (1.11.4)\n",
      "Requirement already satisfied: sentencepiece in /opt/conda/lib/python3.10/site-packages (from tf-models-official==2.15) (0.2.0)\n",
      "Collecting seqeval (from tf-models-official==2.15)\n",
      "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from tf-models-official==2.15) (1.16.0)\n",
      "Requirement already satisfied: tensorflow-datasets in /opt/conda/lib/python3.10/site-packages (from tf-models-official==2.15) (4.9.4)\n",
      "Requirement already satisfied: tensorflow-hub>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from tf-models-official==2.15) (0.16.1)\n",
      "Collecting tensorflow-model-optimization>=0.4.1 (from tf-models-official==2.15)\n",
      "  Downloading tensorflow_model_optimization-0.8.0-py2.py3-none-any.whl.metadata (904 bytes)\n",
      "Requirement already satisfied: tensorflow-text~=2.15.0 in /opt/conda/lib/python3.10/site-packages (from tf-models-official==2.15) (2.15.0)\n",
      "Requirement already satisfied: tensorflow~=2.15.0 in /opt/conda/lib/python3.10/site-packages (from tf-models-official==2.15) (2.15.1)\n",
      "Collecting tf-slim>=1.1.0 (from tf-models-official==2.15)\n",
      "  Downloading tf_slim-1.1.0-py2.py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: httplib2<1.dev0,>=0.19.0 in /opt/conda/lib/python3.10/site-packages (from google-api-python-client>=1.6.7->tf-models-official==2.15) (0.21.0)\n",
      "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0 in /opt/conda/lib/python3.10/site-packages (from google-api-python-client>=1.6.7->tf-models-official==2.15) (2.26.1)\n",
      "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /opt/conda/lib/python3.10/site-packages (from google-api-python-client>=1.6.7->tf-models-official==2.15) (0.2.0)\n",
      "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5 in /opt/conda/lib/python3.10/site-packages (from google-api-python-client>=1.6.7->tf-models-official==2.15) (2.11.1)\n",
      "Requirement already satisfied: uritemplate<5,>=3.0.1 in /opt/conda/lib/python3.10/site-packages (from google-api-python-client>=1.6.7->tf-models-official==2.15) (3.0.1)\n",
      "Requirement already satisfied: certifi>=2023.7.22 in /opt/conda/lib/python3.10/site-packages (from kaggle>=1.3.9->tf-models-official==2.15) (2024.2.2)\n",
      "Requirement already satisfied: python-dateutil in /opt/conda/lib/python3.10/site-packages (from kaggle>=1.3.9->tf-models-official==2.15) (2.9.0.post0)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from kaggle>=1.3.9->tf-models-official==2.15) (2.32.3)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from kaggle>=1.3.9->tf-models-official==2.15) (4.66.4)\n",
      "Requirement already satisfied: python-slugify in /opt/conda/lib/python3.10/site-packages (from kaggle>=1.3.9->tf-models-official==2.15) (8.0.4)\n",
      "Requirement already satisfied: urllib3 in /opt/conda/lib/python3.10/site-packages (from kaggle>=1.3.9->tf-models-official==2.15) (1.26.18)\n",
      "Requirement already satisfied: bleach in /opt/conda/lib/python3.10/site-packages (from kaggle>=1.3.9->tf-models-official==2.15) (6.1.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=0.22.0->tf-models-official==2.15) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas>=0.22.0->tf-models-official==2.15) (2023.4)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow~=2.15.0->tf-models-official==2.15) (1.4.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow~=2.15.0->tf-models-official==2.15) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in /opt/conda/lib/python3.10/site-packages (from tensorflow~=2.15.0->tf-models-official==2.15) (23.5.26)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow~=2.15.0->tf-models-official==2.15) (0.5.4)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow~=2.15.0->tf-models-official==2.15) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow~=2.15.0->tf-models-official==2.15) (3.10.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow~=2.15.0->tf-models-official==2.15) (16.0.6)\n",
      "Requirement already satisfied: ml-dtypes~=0.3.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow~=2.15.0->tf-models-official==2.15) (0.3.2)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.10/site-packages (from tensorflow~=2.15.0->tf-models-official==2.15) (3.3.0)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from tensorflow~=2.15.0->tf-models-official==2.15) (21.3)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow~=2.15.0->tf-models-official==2.15) (3.20.3)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from tensorflow~=2.15.0->tf-models-official==2.15) (69.0.3)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow~=2.15.0->tf-models-official==2.15) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.10/site-packages (from tensorflow~=2.15.0->tf-models-official==2.15) (4.9.0)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow~=2.15.0->tf-models-official==2.15) (1.14.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow~=2.15.0->tf-models-official==2.15) (0.35.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow~=2.15.0->tf-models-official==2.15) (1.59.3)\n",
      "Requirement already satisfied: tensorboard<2.16,>=2.15 in /opt/conda/lib/python3.10/site-packages (from tensorflow~=2.15.0->tf-models-official==2.15) (2.15.1)\n",
      "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow~=2.15.0->tf-models-official==2.15) (2.15.0)\n",
      "Requirement already satisfied: keras<2.16,>=2.15.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow~=2.15.0->tf-models-official==2.15) (2.15.0)\n",
      "Requirement already satisfied: tf-keras>=2.14.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow-hub>=0.6.0->tf-models-official==2.15) (2.15.1)\n",
      "Requirement already satisfied: dm-tree~=0.1.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow-model-optimization>=0.4.1->tf-models-official==2.15) (0.1.8)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->tf-models-official==2.15) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib->tf-models-official==2.15) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->tf-models-official==2.15) (4.47.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->tf-models-official==2.15) (1.4.5)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->tf-models-official==2.15) (3.1.1)\n",
      "Requirement already satisfied: pyasn1>=0.1.7 in /opt/conda/lib/python3.10/site-packages (from oauth2client->tf-models-official==2.15) (0.5.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.0.5 in /opt/conda/lib/python3.10/site-packages (from oauth2client->tf-models-official==2.15) (0.3.0)\n",
      "Requirement already satisfied: rsa>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from oauth2client->tf-models-official==2.15) (4.9)\n",
      "Collecting portalocker (from sacrebleu->tf-models-official==2.15)\n",
      "  Downloading portalocker-2.10.0-py3-none-any.whl.metadata (8.5 kB)\n",
      "Requirement already satisfied: regex in /opt/conda/lib/python3.10/site-packages (from sacrebleu->tf-models-official==2.15) (2023.12.25)\n",
      "Requirement already satisfied: tabulate>=0.8.9 in /opt/conda/lib/python3.10/site-packages (from sacrebleu->tf-models-official==2.15) (0.9.0)\n",
      "Requirement already satisfied: colorama in /opt/conda/lib/python3.10/site-packages (from sacrebleu->tf-models-official==2.15) (0.4.6)\n",
      "Requirement already satisfied: lxml in /opt/conda/lib/python3.10/site-packages (from sacrebleu->tf-models-official==2.15) (5.2.2)\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in /opt/conda/lib/python3.10/site-packages (from seqeval->tf-models-official==2.15) (1.2.2)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.10/site-packages (from tensorflow-datasets->tf-models-official==2.15) (8.1.7)\n",
      "Requirement already satisfied: etils>=0.9.0 in /opt/conda/lib/python3.10/site-packages (from etils[enp,epath,etree]>=0.9.0->tensorflow-datasets->tf-models-official==2.15) (1.6.0)\n",
      "Requirement already satisfied: promise in /opt/conda/lib/python3.10/site-packages (from tensorflow-datasets->tf-models-official==2.15) (2.3)\n",
      "Requirement already satisfied: tensorflow-metadata in /opt/conda/lib/python3.10/site-packages (from tensorflow-datasets->tf-models-official==2.15) (0.14.0)\n",
      "Requirement already satisfied: toml in /opt/conda/lib/python3.10/site-packages (from tensorflow-datasets->tf-models-official==2.15) (0.10.2)\n",
      "Requirement already satisfied: array-record>=0.5.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow-datasets->tf-models-official==2.15) (0.5.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow~=2.15.0->tf-models-official==2.15) (0.42.0)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from etils[enp,epath,etree]>=0.9.0->tensorflow-datasets->tf-models-official==2.15) (2024.3.1)\n",
      "Requirement already satisfied: importlib_resources in /opt/conda/lib/python3.10/site-packages (from etils[enp,epath,etree]>=0.9.0->tensorflow-datasets->tf-models-official==2.15) (6.1.1)\n",
      "Requirement already satisfied: zipp in /opt/conda/lib/python3.10/site-packages (from etils[enp,epath,etree]>=0.9.0->tensorflow-datasets->tf-models-official==2.15) (3.17.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /opt/conda/lib/python3.10/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client>=1.6.7->tf-models-official==2.15) (1.62.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0->google-api-python-client>=1.6.7->tf-models-official==2.15) (4.2.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->kaggle>=1.3.9->tf-models-official==2.15) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->kaggle>=1.3.9->tf-models-official==2.15) (3.6)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.21.3->seqeval->tf-models-official==2.15) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.21.3->seqeval->tf-models-official==2.15) (3.2.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-models-official==2.15) (1.2.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-models-official==2.15) (3.5.2)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-models-official==2.15) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-models-official==2.15) (3.0.3)\n",
      "Requirement already satisfied: webencodings in /opt/conda/lib/python3.10/site-packages (from bleach->kaggle>=1.3.9->tf-models-official==2.15) (0.5.1)\n",
      "Requirement already satisfied: text-unidecode>=1.3 in /opt/conda/lib/python3.10/site-packages (from python-slugify->kaggle>=1.3.9->tf-models-official==2.15) (1.3)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-models-official==2.15) (1.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-models-official==2.15) (2.1.3)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-models-official==2.15) (3.2.2)\n",
      "Downloading tf_models_official-2.15.0-py2.py3-none-any.whl (2.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m60.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tensorflow_model_optimization-0.8.0-py2.py3-none-any.whl (242 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.5/242.5 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tf_slim-1.1.0-py2.py3-none-any.whl (352 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m352.1/352.1 kB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading gin_config-0.5.0-py3-none-any.whl (61 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.3/61.3 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading immutabledict-4.2.0-py3-none-any.whl (4.7 kB)\n",
      "Downloading pycocotools-2.0.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (427 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m427.8/427.8 kB\u001b[0m \u001b[31m25.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading sacrebleu-2.4.2-py3-none-any.whl (106 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.7/106.7 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading portalocker-2.10.0-py3-none-any.whl (18 kB)\n",
      "Building wheels for collected packages: seqeval\n",
      "  Building wheel for seqeval (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16162 sha256=55b1ee3b345c0898b313df9d09ce7b8e78be1019f5dea4e635e814370bf79ec9\n",
      "  Stored in directory: /root/.cache/pip/wheels/1a/67/4a/ad4082dd7dfc30f2abfe4d80a2ed5926a506eb8a972b4767fa\n",
      "Successfully built seqeval\n",
      "Installing collected packages: gin-config, tf-slim, tensorflow-model-optimization, portalocker, immutabledict, sacrebleu, seqeval, pycocotools, tf-models-official\n",
      "Successfully installed gin-config-0.5.0 immutabledict-4.2.0 portalocker-2.10.0 pycocotools-2.0.8 sacrebleu-2.4.2 seqeval-1.2.2 tensorflow-model-optimization-0.8.0 tf-models-official-2.15.0 tf-slim-1.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install tf-models-official==2.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-11T15:31:09.292427Z",
     "iopub.status.busy": "2024-07-11T15:31:09.291499Z",
     "iopub.status.idle": "2024-07-11T15:31:09.299261Z",
     "shell.execute_reply": "2024-07-11T15:31:09.298316Z",
     "shell.execute_reply.started": "2024-07-11T15:31:09.292383Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.15.1'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.version.VERSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-11T15:35:56.515084Z",
     "iopub.status.busy": "2024-07-11T15:35:56.514193Z",
     "iopub.status.idle": "2024-07-11T15:35:56.524679Z",
     "shell.execute_reply": "2024-07-11T15:35:56.523794Z",
     "shell.execute_reply.started": "2024-07-11T15:35:56.515052Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import random\n",
    "import numpy as np\n",
    "import os, glob\n",
    "from tensorflow.keras.layers import Flatten, Dense, BatchNormalization, Input\n",
    "from tensorflow.keras.models import Model\n",
    "from PIL import Image\n",
    "\n",
    "gpus=tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu,True)\n",
    "print(gpus)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Ensure the same seed for reproducibility\n",
    "random.seed(12)\n",
    "np.random.seed(12)\n",
    "tf.random.set_seed(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-11T15:35:59.146044Z",
     "iopub.status.busy": "2024-07-11T15:35:59.145675Z",
     "iopub.status.idle": "2024-07-11T15:35:59.155601Z",
     "shell.execute_reply": "2024-07-11T15:35:59.154542Z",
     "shell.execute_reply.started": "2024-07-11T15:35:59.146016Z"
    }
   },
   "outputs": [],
   "source": [
    "# If you have any custom objects (e.g., custom metrics or layers), define/import them here.\n",
    "class AngularError(tf.keras.metrics.Metric):\n",
    "    def __init__(self, name='mean_angular_error', **kwargs):\n",
    "        super().__init__(name=name, **kwargs)\n",
    "        self.total_error = self.add_weight(name='total_error', initializer='zeros')\n",
    "        self.num_samples = self.add_weight(name='num_samples', initializer='zeros')\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_true = tf.math.l2_normalize(y_true, axis=-1)\n",
    "        y_pred = tf.math.l2_normalize(y_pred, axis=-1)\n",
    "        dot_product = tf.reduce_sum(y_true * y_pred, axis=-1)\n",
    "        dot_product = tf.clip_by_value(dot_product, -1.0, 1.0)\n",
    "        angular_error = tf.acos(dot_product)\n",
    "        angular_error = angular_error * 57.296\n",
    "        self.total_error.assign_add(tf.reduce_sum(angular_error))\n",
    "        self.num_samples.assign_add(tf.cast(tf.shape(y_true)[0], tf.float32))\n",
    "\n",
    "    def result(self):\n",
    "        return self.total_error / self.num_samples\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.total_error.assign(0.0)\n",
    "        self.num_samples.assign(0.0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPLEMENTING THE GAZE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-11T15:36:53.175707Z",
     "iopub.status.busy": "2024-07-11T15:36:53.174988Z",
     "iopub.status.idle": "2024-07-11T15:37:00.137629Z",
     "shell.execute_reply": "2024-07-11T15:37:00.136838Z",
     "shell.execute_reply.started": "2024-07-11T15:36:53.175675Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/efficientnet_v2/efficientnetv2-b0_notop.h5\n",
      "24274472/24274472 [==============================] - 1s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "58889256/58889256 [==============================] - 2s 0us/step\n"
     ]
    }
   ],
   "source": [
    "# Clear any previous session\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# Load pretrained EfficientNetV2 for face feature extraction\n",
    "effcnt_net = tf.keras.applications.EfficientNetV2B0(include_top=False, include_preprocessing=True, pooling=None)\n",
    "effcnt_net.trainable = True\n",
    "g_face = Model(inputs=effcnt_net.inputs, outputs=effcnt_net.outputs, name='g_face')\n",
    "\n",
    "# Load pretrained VGG16 for eye feature extraction\n",
    "vgg16 = tf.keras.applications.VGG16(include_top=False, pooling=None)\n",
    "vgg16.trainable = True\n",
    "vgg16_processor = tf.keras.applications.vgg16.preprocess_input\n",
    "g_eye = Model(inputs=vgg16.inputs, outputs=vgg16.outputs, name='g_eye')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-11T15:37:45.278205Z",
     "iopub.status.busy": "2024-07-11T15:37:45.277492Z",
     "iopub.status.idle": "2024-07-11T16:14:18.407822Z",
     "shell.execute_reply": "2024-07-11T16:14:18.406856Z",
     "shell.execute_reply.started": "2024-07-11T15:37:45.278157Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Leaving out subject: p00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1720712272.547552      34 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/2], Loss: 5.751259233853314\n",
      "Validation Loss: 0.007881487585604191\n",
      "Epoch [2/2], Loss: 0.02611466482964655\n",
      "Validation Loss: 0.004395916781077782\n",
      "Gaze model weights for subject p00 saved.\n",
      "Validation Loss for subject p00: 0.004395916781077782\n",
      "Average Validation Loss: 0.004395916781077782\n"
     ]
    }
   ],
   "source": [
    "# this did training for Gazer\n",
    "\n",
    "# Define the Gaze Model using the pretrained models\n",
    "class GazeModel(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(GazeModel, self).__init__()\n",
    "        self.g_face = g_face\n",
    "        self.g_eye = g_eye\n",
    "        self.flat = Flatten()\n",
    "        self.embedding = tf.keras.layers.Embedding(15, 6, embeddings_regularizer=tf.keras.regularizers.L2(0.01), mask_zero=True, name='subject_embedding')\n",
    "        self.embedding.trainable = False\n",
    "        self.MLP = tf.keras.Sequential([\n",
    "            Dense(1280, activation='relu'),\n",
    "            BatchNormalization(),\n",
    "            Dense(3, name='gaze_location')\n",
    "        ], name='MLP')\n",
    "\n",
    "    def call(self, input_dict):\n",
    "        # Transpose to (batch_size, height, width, channels)\n",
    "        face_image = input_dict['face']\n",
    "        flipped_face_image = tf.image.flip_left_right(face_image)\n",
    "        left_eye_image = input_dict['lefteye']\n",
    "        right_eye_image = input_dict['righteye']\n",
    "\n",
    "        face_features = self.g_face(face_image)\n",
    "        flipped_face_features = self.g_face(flipped_face_image)\n",
    "        left_features = self.g_eye(left_eye_image)\n",
    "        right_features = self.g_eye(right_eye_image)\n",
    "\n",
    "        face_features = self.flat(face_features)\n",
    "        flipped_face_features = self.flat(flipped_face_features)\n",
    "        left_features = self.flat(left_features)\n",
    "        right_features = self.flat(right_features)\n",
    "        \n",
    "        embedding = self.embedding(input_dict['id'])\n",
    "        rot_mat = tf.reshape(input_dict['rotation_matrix'], [tf.shape(input_dict['rotation_matrix'])[0], -1])\n",
    "        eye_coords = tf.reshape(input_dict['eye_coords'], [tf.shape(input_dict['eye_coords'])[0], -1])\n",
    "        \n",
    "        total = tf.concat([face_features, flipped_face_features, left_features,\n",
    "                           right_features, embedding, rot_mat, eye_coords], 1)\n",
    "        total = self.MLP(total)\n",
    "        return total\n",
    "\n",
    "# Define transformations using TensorFlow\n",
    "def preprocess_image(image, target_size):\n",
    "    image = tf.image.resize(image, target_size)\n",
    "    image = tf.keras.applications.efficientnet_v2.preprocess_input(image)\n",
    "    return image\n",
    "\n",
    "class GazeDataset(tf.data.Dataset):\n",
    "    def __new__(cls, subject_to_leave_out=None, batch_size=8, validation=False):\n",
    "        def _generator():\n",
    "            root_dir = 'processed_data'\n",
    "            # TODO: Use the full list of subject ids if proper compute resources available\n",
    "            subjects = ['p00', 'p01', 'p02']\n",
    "            transform_face = lambda img: preprocess_image(img, (224, 224))\n",
    "            transform_eye = lambda img: preprocess_image(img, (112, 112))\n",
    "\n",
    "            for subject in subjects:\n",
    "                if validation:\n",
    "                    if subject != subject_to_leave_out:\n",
    "                        continue\n",
    "                else:\n",
    "                    if subject == subject_to_leave_out:\n",
    "                        continue\n",
    "                \n",
    "                person_dir = os.path.join(root_dir, 'Image', subject)\n",
    "                for image_name in os.listdir(os.path.join(person_dir, 'face')):\n",
    "                    face_image_path = os.path.join(person_dir, 'face', image_name)\n",
    "                    left_eye_image_path = os.path.join(person_dir, 'lefteye', image_name)\n",
    "                    right_eye_image_path = os.path.join(person_dir, 'righteye', image_name)\n",
    "                    rotation_matrix_path = os.path.join(person_dir, 'rotation_matrix', image_name.replace('.jpg', '.npy'))\n",
    "                    rotation_matrix_flipped_path = os.path.join(person_dir, 'rotation_matrix_flipped', image_name.replace('.jpg', '.npy'))\n",
    "                    gaze_2d_path = os.path.join(person_dir, '2d_gaze', image_name.replace('.jpg', '.npy'))\n",
    "                    gaze_3d_path = os.path.join(person_dir, '3d_gaze', image_name.replace('.jpg', '.npy'))\n",
    "                    gaze_3d_flipped_path = os.path.join(person_dir, '3d_gaze_flipped', image_name.replace('.jpg', '.npy'))\n",
    "                    eye_coords_path = os.path.join(person_dir, 'eye_coords', image_name.replace('.jpg', '.npy'))\n",
    "\n",
    "                    face_image = Image.open(face_image_path).convert('RGB')\n",
    "                    left_eye_image = Image.open(left_eye_image_path).convert('RGB')\n",
    "                    right_eye_image = Image.open(right_eye_image_path).convert('RGB')\n",
    "\n",
    "                    face_image = transform_face(np.array(face_image))\n",
    "                    left_eye_image = transform_eye(np.array(left_eye_image))\n",
    "                    right_eye_image = transform_eye(np.array(right_eye_image))\n",
    "\n",
    "                    rotation_matrix = np.load(rotation_matrix_path)\n",
    "                    rotation_matrix_flipped = np.load(rotation_matrix_flipped_path)\n",
    "                    gaze_2d = np.load(gaze_2d_path)\n",
    "                    gaze_3d = np.load(gaze_3d_path)\n",
    "                    gaze_3d_flipped = np.load(gaze_3d_flipped_path)\n",
    "                    eye_coords = np.load(eye_coords_path)\n",
    "\n",
    "                    yield face_image, left_eye_image, right_eye_image, rotation_matrix, rotation_matrix_flipped, gaze_2d, gaze_3d, gaze_3d_flipped, eye_coords, subject\n",
    "\n",
    "        return tf.data.Dataset.from_generator(\n",
    "            _generator,\n",
    "            output_signature=(\n",
    "                tf.TensorSpec(shape=(224, 224, 3), dtype=tf.float32),\n",
    "                tf.TensorSpec(shape=(112, 112, 3), dtype=tf.float32),\n",
    "                tf.TensorSpec(shape=(112, 112, 3), dtype=tf.float32),\n",
    "                tf.TensorSpec(shape=(3, 3), dtype=tf.float32),\n",
    "                tf.TensorSpec(shape=(3, 3), dtype=tf.float32),\n",
    "                tf.TensorSpec(shape=(2,), dtype=tf.float32),\n",
    "                tf.TensorSpec(shape=(3,), dtype=tf.float32),\n",
    "                tf.TensorSpec(shape=(3,), dtype=tf.float32),\n",
    "                tf.TensorSpec(shape=(6,), dtype=tf.float32),\n",
    "                tf.TensorSpec(shape=(), dtype=tf.string)\n",
    "            )\n",
    "        ).batch(batch_size)\n",
    "# Define custom Huber loss function\n",
    "def custom_huber_loss(y_true, y_pred, delta=1.5):\n",
    "    error = y_true - y_pred\n",
    "    is_small_error = tf.abs(error) <= delta\n",
    "\n",
    "    small_error_loss = tf.square(error) / 2\n",
    "    big_error_loss = delta * (tf.abs(error) - delta / 2)\n",
    "\n",
    "    return tf.where(is_small_error, small_error_loss, big_error_loss)\n",
    "\n",
    "# Calculate gaze loss as the average of Huber losses\n",
    "def gaze_loss(y_true, y_pred, delta=1.5):\n",
    "    huber_losses = custom_huber_loss(y_true, y_pred, delta)\n",
    "    return tf.reduce_mean(huber_losses)\n",
    "\n",
    "# Train the Gaze Model\n",
    "def train_gaze_model(model, train_dataloader, val_dataloader, optimizer, num_epochs=2):\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        num_batches = 0\n",
    "        for i, (face_image, left_eye_image, right_eye_image, rotation_matrix, rotation_matrix_flipped, gaze_2d, gaze_3d, gaze_3d_flipped, eye_coords, subject_id) in enumerate(train_dataloader):\n",
    "            with tf.GradientTape() as tape:\n",
    "                input_dict = {\n",
    "                    'face': face_image,\n",
    "                    'flipped_face': tf.image.flip_left_right(face_image),\n",
    "                    'lefteye': left_eye_image,\n",
    "                    'righteye': right_eye_image,\n",
    "                    'id': tf.constant([0 if s == 'p00' else 1 if s == 'p01' else 2 for s in subject_id], dtype=tf.int32),\n",
    "                    'rotation_matrix': rotation_matrix,\n",
    "                    'eye_coords': eye_coords\n",
    "                }\n",
    "                outputs = model(input_dict)\n",
    "                #print(\"training\")\n",
    "                loss = gaze_loss(gaze_3d, outputs)\n",
    "            grads = tape.gradient(loss, model.trainable_weights)\n",
    "            optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "            running_loss += loss.numpy()\n",
    "            num_batches += 1\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/num_batches}\")\n",
    "\n",
    "        val_loss = 0.0\n",
    "        num_val_batches = 0\n",
    "        for i, (face_image, left_eye_image, right_eye_image, rotation_matrix, rotation_matrix_flipped, gaze_2d, gaze_3d, gaze_3d_flipped, eye_coords, subject_id) in enumerate(val_dataloader):\n",
    "            input_dict = {\n",
    "                'face': face_image,\n",
    "                'flipped_face': tf.image.flip_left_right(face_image),\n",
    "                'lefteye': left_eye_image,\n",
    "                'righteye': right_eye_image,\n",
    "                'id': tf.constant([0 if s == 'p00' else 1 if s == 'p01' else 2 for s in subject_id], dtype=tf.int32),\n",
    "                'rotation_matrix': rotation_matrix,\n",
    "                'eye_coords': eye_coords\n",
    "            }\n",
    "            outputs = model(input_dict)\n",
    "            loss = gaze_loss(gaze_3d, outputs)\n",
    "            val_loss += loss.numpy()\n",
    "            num_val_batches += 1\n",
    "        print(f\"Validation Loss: {val_loss/num_val_batches}\")\n",
    "\n",
    "# Implementing the leave-one-out strategy\n",
    "\n",
    "# TODO\n",
    "#subjects = ['p00', 'p01', 'p02']\n",
    "# Due to less compute resources and storage, we are leaving out only one subject\n",
    "subjects = ['p00']\n",
    "validation_losses = []\n",
    "\n",
    "for subject in subjects:\n",
    "    print(f\"Leaving out subject: {subject}\")\n",
    "    train_dataloader = GazeDataset(subject_to_leave_out=subject, batch_size=8, validation=False)\n",
    "    val_dataloader = GazeDataset(subject_to_leave_out=subject, batch_size=8, validation=True)\n",
    "    \n",
    "    # Initialize models and optimizer\n",
    "    gaze_model = GazeModel()\n",
    "    gaze_optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "    \n",
    "    # Train the gaze model\n",
    "    train_gaze_model(gaze_model, train_dataloader, val_dataloader, gaze_optimizer, num_epochs=2)\n",
    "    \n",
    "    # Save the GazeModel weights\n",
    "    gaze_model.save_weights(f'gaze_model_weights_{subject}')\n",
    "    print(f\"Gaze model weights for subject {subject} saved.\")\n",
    "    \n",
    "    # Validate the model and store the loss\n",
    "    val_loss = 0.0\n",
    "    num_val_batches = 0\n",
    "    for i, (face_image, left_eye_image, right_eye_image, rotation_matrix, rotation_matrix_flipped, gaze_2d, gaze_3d, gaze_3d_flipped, eye_coords, subject_id) in enumerate(val_dataloader):\n",
    "        input_dict = {\n",
    "            'face': face_image,\n",
    "            'flipped_face': tf.image.flip_left_right(face_image),\n",
    "            'lefteye': left_eye_image,\n",
    "            'righteye': right_eye_image,\n",
    "            'id': tf.constant([0 if s == 'p00' else 1 if s == 'p01' else 2 for s in subject_id], dtype=tf.int32),\n",
    "            'rotation_matrix': rotation_matrix,\n",
    "            'eye_coords': eye_coords\n",
    "        }\n",
    "        outputs = gaze_model(input_dict)\n",
    "        #print(\"validating\")\n",
    "        loss = gaze_loss(gaze_3d, outputs)\n",
    "        val_loss += loss.numpy()\n",
    "        num_val_batches += 1\n",
    "    validation_losses.append(val_loss / num_val_batches)\n",
    "    print(f\"Validation Loss for subject {subject}: {val_loss / num_val_batches}\")\n",
    "\n",
    "# Calculate the average validation loss\n",
    "average_val_loss = sum(validation_losses) / len(validation_losses)\n",
    "print(f\"Average Validation Loss: {average_val_loss}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPLEMENTING THE CALIBRATION MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-11T16:14:53.085369Z",
     "iopub.status.busy": "2024-07-11T16:14:53.084958Z",
     "iopub.status.idle": "2024-07-11T16:52:33.899727Z",
     "shell.execute_reply": "2024-07-11T16:52:33.898752Z",
     "shell.execute_reply.started": "2024-07-11T16:14:53.085336Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Leaving out subject: p00\n",
      "Epoch [1/2], Loss: 0.04863577569834888\n",
      "Epoch [2/2], Loss: 0.005534778931876644\n",
      "Validation Loss for subject p00: 0.003976876734445492\n",
      "Overall Performance: 0.003976876734445492\n",
      "Calibration model weights saved.\n"
     ]
    }
   ],
   "source": [
    "# this did training for calibratiobn\n",
    "\n",
    "# Calibration Dataset Class\n",
    "class CalibrationDataset(tf.data.Dataset):\n",
    "    def __new__(cls, subject_to_leave_out=None, batch_size=8, validation=False):\n",
    "        def _generator():\n",
    "            root_dir = 'processed_data'\n",
    "            # TODO: Use the full list of subject ids if proper compute resources available\n",
    "            subjects = ['p00', 'p01', 'p02']\n",
    "            transform_face = lambda img: preprocess_image(img, (224, 224))\n",
    "            transform_eye = lambda img: preprocess_image(img, (112, 112))\n",
    "\n",
    "            for subject in subjects:\n",
    "                if validation:\n",
    "                    if subject != subject_to_leave_out:\n",
    "                        continue\n",
    "                else:\n",
    "                    if subject == subject_to_leave_out:\n",
    "                        continue\n",
    "                \n",
    "                person_dir = os.path.join(root_dir, 'Image', subject)\n",
    "                for image_name in os.listdir(os.path.join(person_dir, 'face')):\n",
    "                    face_image_path = os.path.join(person_dir, 'face', image_name)\n",
    "                    left_eye_image_path = os.path.join(person_dir, 'lefteye', image_name)\n",
    "                    right_eye_image_path = os.path.join(person_dir, 'righteye', image_name)\n",
    "                    rotation_matrix_path = os.path.join(person_dir, 'rotation_matrix', image_name.replace('.jpg', '.npy'))\n",
    "                    rotation_matrix_flipped_path = os.path.join(person_dir, 'rotation_matrix_flipped', image_name.replace('.jpg', '.npy'))\n",
    "                    gaze_2d_path = os.path.join(person_dir, '2d_gaze', image_name.replace('.jpg', '.npy'))\n",
    "                    gaze_3d_path = os.path.join(person_dir, '3d_gaze', image_name.replace('.jpg', '.npy'))\n",
    "                    gaze_3d_flipped_path = os.path.join(person_dir, '3d_gaze_flipped', image_name.replace('.jpg', '.npy'))\n",
    "                    eye_coords_path = os.path.join(person_dir, 'eye_coords', image_name.replace('.jpg', '.npy'))\n",
    "\n",
    "                    face_image = Image.open(face_image_path).convert('RGB')\n",
    "                    left_eye_image = Image.open(left_eye_image_path).convert('RGB')\n",
    "                    right_eye_image = Image.open(right_eye_image_path).convert('RGB')\n",
    "\n",
    "                    face_image = transform_face(np.array(face_image))\n",
    "                    left_eye_image = transform_eye(np.array(left_eye_image))\n",
    "                    right_eye_image = transform_eye(np.array(right_eye_image))\n",
    "\n",
    "                    rotation_matrix = np.load(rotation_matrix_path)\n",
    "                    rotation_matrix_flipped = np.load(rotation_matrix_flipped_path)\n",
    "                    gaze_2d = np.load(gaze_2d_path)\n",
    "                    gaze_3d = np.load(gaze_3d_path)\n",
    "                    gaze_3d_flipped = np.load(gaze_3d_flipped_path)\n",
    "                    eye_coords = np.load(eye_coords_path)\n",
    "\n",
    "                    yield face_image, left_eye_image, right_eye_image, rotation_matrix, rotation_matrix_flipped, gaze_2d, gaze_3d, gaze_3d_flipped, eye_coords, subject\n",
    "\n",
    "        return tf.data.Dataset.from_generator(\n",
    "            _generator,\n",
    "            output_signature=(\n",
    "                tf.TensorSpec(shape=(224, 224, 3), dtype=tf.float32),\n",
    "                tf.TensorSpec(shape=(112, 112, 3), dtype=tf.float32),\n",
    "                tf.TensorSpec(shape=(112, 112, 3), dtype=tf.float32),\n",
    "                tf.TensorSpec(shape=(3, 3), dtype=tf.float32),\n",
    "                tf.TensorSpec(shape=(3, 3), dtype=tf.float32),\n",
    "                tf.TensorSpec(shape=(2,), dtype=tf.float32),\n",
    "                tf.TensorSpec(shape=(3,), dtype=tf.float32),\n",
    "                tf.TensorSpec(shape=(3,), dtype=tf.float32),\n",
    "                tf.TensorSpec(shape=(6,), dtype=tf.float32),\n",
    "                tf.TensorSpec(shape=(), dtype=tf.string)\n",
    "            )\n",
    "        ).batch(batch_size)\n",
    "\n",
    "# Define the Calibration Model\n",
    "class CalibrationModel(tf.keras.Model):\n",
    "    def __init__(self, gaze_model):\n",
    "        super(CalibrationModel, self).__init__()\n",
    "        self.g_face = gaze_model.g_face\n",
    "        self.g_eye = gaze_model.g_eye\n",
    "        self.transformer_stack = transformer\n",
    "        self.flat = Flatten()\n",
    "        self.MLP1 = tf.keras.Sequential([\n",
    "            Dense(1280, activation='relu'),\n",
    "            BatchNormalization(),\n",
    "        ], name='MLP1')\n",
    "        self.MLP2 = tf.keras.Sequential([\n",
    "            Dense(1280, activation='relu'),\n",
    "            BatchNormalization(),\n",
    "        ], name='MLP2')\n",
    "        self.output_layer = Dense(6, name='subject_feature')\n",
    "\n",
    "    def call(self, input_dict):\n",
    "        face_features = self.g_face(input_dict['face'])\n",
    "        flipped_face_features = self.g_face(input_dict['flipped_face'])\n",
    "        left_features = self.g_eye(input_dict['lefteye'])\n",
    "        right_features = self.g_eye(input_dict['righteye'])\n",
    "\n",
    "        face_features = self.flat(face_features)\n",
    "        flipped_face_features = self.flat(flipped_face_features)\n",
    "        left_features = self.flat(left_features)\n",
    "        right_features = self.flat(right_features)\n",
    "\n",
    "        rot_mat = tf.reshape(input_dict['rotation_matrix'], [tf.shape(input_dict['rotation_matrix'])[0], -1])\n",
    "        rot_mat_flipped = tf.reshape(input_dict['rotation_matrix_flipped'], [tf.shape(input_dict['rotation_matrix_flipped'])[0], -1])\n",
    "        eye_coords = tf.reshape(input_dict['eye_coords'], [tf.shape(input_dict['eye_coords'])[0], -1])\n",
    "        gaze = input_dict['gaze']\n",
    "        gaze_flipped = input_dict['gaze_flipped']\n",
    "\n",
    "        total = tf.concat([face_features, flipped_face_features, left_features,\n",
    "                           right_features, eye_coords, rot_mat, rot_mat_flipped,\n",
    "                           gaze, gaze_flipped], 1)\n",
    "        \n",
    "        total = self.MLP1(total)\n",
    "        total = tf.expand_dims(total, axis=1)\n",
    "        total = self.transformer_stack(total)\n",
    "        total = self.MLP2(tf.squeeze(total, 1))\n",
    "        final_output = self.output_layer(total)\n",
    "        return final_output\n",
    "\n",
    "# Training function for Calibration Model\n",
    "def train_calibration_model(model, dataloader, gaze_model, optimizer, num_epochs=2):\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        num_batches = 0\n",
    "        for i, (face_image, left_eye_image, right_eye_image, rotation_matrix, rotation_matrix_flipped, gaze_2d, gaze_3d, gaze_3d_flipped, eye_coords, subject_id) in enumerate(dataloader):\n",
    "            with tf.GradientTape() as tape:\n",
    "                input_dict = {\n",
    "                    'face': face_image,\n",
    "                    'flipped_face': tf.image.flip_left_right(face_image),\n",
    "                    'lefteye': left_eye_image,\n",
    "                    'righteye': right_eye_image,\n",
    "                    'id': tf.constant([0 if s == 'p00' else 1 if s == 'p01' else 2 for s in subject_id], dtype=tf.int32),\n",
    "                    'rotation_matrix': rotation_matrix,\n",
    "                    'rotation_matrix_flipped': rotation_matrix_flipped,\n",
    "                    'eye_coords': eye_coords,\n",
    "                    'gaze': gaze_3d,\n",
    "                    'gaze_flipped': gaze_3d_flipped\n",
    "                }\n",
    "                predicted_embeddings = model(input_dict)\n",
    "                #print(\"training\")\n",
    "                # Accuracy loss\n",
    "                preference_vectors = gaze_model.embedding(input_dict['id'])\n",
    "                accuracy_loss = tf.reduce_mean(tf.square(predicted_embeddings - preference_vectors))\n",
    "                \n",
    "                # Consistency loss\n",
    "                consistency_loss = embedding_consistency_loss(predicted_embeddings, batch_size=8)\n",
    "                \n",
    "                # Total loss\n",
    "                loss = accuracy_loss + consistency_loss\n",
    "                \n",
    "            grads = tape.gradient(loss, model.trainable_weights)\n",
    "            optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "            running_loss += loss.numpy()\n",
    "            num_batches += 1\n",
    "        \n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/num_batches}\")\n",
    "\n",
    "# Implementing the leave-one-out strategy for Calibration Model\n",
    "validation_losses = []\n",
    "# Initialize the transformer encoder\n",
    "import tensorflow as tf\n",
    "import tensorflow_models as tfm\n",
    "\n",
    "transformer = tfm.nlp.models.TransformerEncoder(\n",
    "    num_layers=6,\n",
    "    num_attention_heads=4,\n",
    "    intermediate_size=2048,\n",
    "    activation='relu',\n",
    "    dropout_rate=0.0,\n",
    "    attention_dropout_rate=0.0,\n",
    "    use_bias=True,\n",
    "    norm_first=True,\n",
    "    norm_epsilon=1e-06,\n",
    "    intermediate_dropout=0.0,\n",
    ")\n",
    "# Define custom embedding consistency loss\n",
    "def embedding_consistency_loss(embeddings, batch_size):\n",
    "    diff = tf.expand_dims(embeddings, 1) - tf.expand_dims(embeddings, 0)\n",
    "    diff = tf.reduce_sum(tf.square(diff), axis=-1)\n",
    "    diff = tf.sqrt(diff + 1e-12)\n",
    "    consistency_loss = tf.reduce_mean(diff)\n",
    "    return consistency_loss\n",
    "\n",
    "# Define angular difference function\n",
    "def angular_difference(a, b):\n",
    "    dot_product = tf.reduce_sum(a * b, axis=-1)\n",
    "    norm_a = tf.norm(a, axis=-1)\n",
    "    norm_b = tf.norm(b, axis=-1)\n",
    "    cos_theta = dot_product / (norm_a * norm_b)\n",
    "    cos_theta = tf.clip_by_value(cos_theta, -1.0, 1.0)\n",
    "    return tf.acos(cos_theta)\n",
    "\n",
    "for subject in subjects:\n",
    "    print(f\"Leaving out subject: {subject}\")\n",
    "    train_dataloader = CalibrationDataset(subject_to_leave_out=subject, batch_size=8, validation=False)\n",
    "    val_dataloader = CalibrationDataset(subject_to_leave_out=subject, batch_size=8, validation=True)\n",
    "    \n",
    "    # Initialize models and optimizer\n",
    "    gaze_model = GazeModel()\n",
    "    gaze_model.load_weights('gaze_model_weights_p00')  # Load the pre-trained GazeModel weights\n",
    "    calibration_model = CalibrationModel(gaze_model)\n",
    "    calibration_optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "    \n",
    "    # Train the calibration model\n",
    "    train_calibration_model(calibration_model, train_dataloader, gaze_model, calibration_optimizer, num_epochs=2)\n",
    "    \n",
    "    # Evaluate the calibration model\n",
    "    val_loss = 0.0\n",
    "    num_val_batches = 0\n",
    "    for i, (face_image, left_eye_image, right_eye_image, rotation_matrix, rotation_matrix_flipped, gaze_2d, gaze_3d, gaze_3d_flipped, eye_coords, subject_id) in enumerate(val_dataloader):\n",
    "        input_dict = {\n",
    "            'face': face_image,\n",
    "            'flipped_face': tf.image.flip_left_right(face_image),\n",
    "            'lefteye': left_eye_image,\n",
    "            'righteye': right_eye_image,\n",
    "            'id': tf.constant([0 if s == 'p00' else 1 if s == 'p01' else 2 for s in subject_id], dtype=tf.int32),\n",
    "            'rotation_matrix': rotation_matrix,\n",
    "            'rotation_matrix_flipped': rotation_matrix_flipped,\n",
    "            'eye_coords': eye_coords,\n",
    "            'gaze': gaze_3d,\n",
    "            'gaze_flipped': gaze_3d_flipped\n",
    "        }\n",
    "        predicted_embeddings = calibration_model(input_dict)\n",
    "        #print(\"validating\")\n",
    "        # Accuracy loss\n",
    "        preference_vectors = gaze_model.embedding(input_dict['id'])\n",
    "        accuracy_loss = tf.reduce_mean(tf.square(predicted_embeddings - preference_vectors))\n",
    "        \n",
    "        # Consistency loss\n",
    "        consistency_loss = embedding_consistency_loss(predicted_embeddings, batch_size=8)\n",
    "        \n",
    "        # Total loss\n",
    "        loss = accuracy_loss + consistency_loss\n",
    "        val_loss += loss.numpy()\n",
    "        num_val_batches += 1\n",
    "    \n",
    "    validation_losses.append(val_loss / num_val_batches)\n",
    "    print(f\"Validation Loss for subject {subject}: {val_loss/num_val_batches}\")\n",
    "\n",
    "# Calculate overall performance\n",
    "overall_performance = sum(validation_losses) / len(validation_losses)\n",
    "print(f\"Overall Performance: {overall_performance}\")\n",
    "\n",
    "# Save the Calibration Model\n",
    "calibration_model.save_weights('calibration_model_weights')\n",
    "print(\"Calibration model weights saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-11T16:53:17.207758Z",
     "iopub.status.busy": "2024-07-11T16:53:17.206324Z",
     "iopub.status.idle": "2024-07-11T16:56:39.118715Z",
     "shell.execute_reply": "2024-07-11T16:56:39.117809Z",
     "shell.execute_reply.started": "2024-07-11T16:53:17.207726Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance with Corrupted Data: 0.003977099084605773\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Function to introduce noise to images\n",
    "def add_noise(image, noise_factor=0.1):\n",
    "    noisy_image = image + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=image.shape)\n",
    "    noisy_image = np.clip(noisy_image, 0.0, 1.0)\n",
    "    return noisy_image\n",
    "\n",
    "# Function to create a corrupted validation set\n",
    "def create_corrupted_dataset(dataloader, noise_factor=0.1):\n",
    "    corrupted_data = []\n",
    "    for i, (face_image, left_eye_image, right_eye_image, rotation_matrix, rotation_matrix_flipped, gaze_2d, gaze_3d, gaze_3d_flipped, eye_coords, subject_id) in enumerate(dataloader):\n",
    "        corrupted_face_image = add_noise(face_image, noise_factor)\n",
    "        corrupted_left_eye_image = add_noise(left_eye_image, noise_factor)\n",
    "        corrupted_right_eye_image = add_noise(right_eye_image, noise_factor)\n",
    "        corrupted_data.append((corrupted_face_image, corrupted_left_eye_image, corrupted_right_eye_image, rotation_matrix, rotation_matrix_flipped, gaze_2d, gaze_3d, gaze_3d_flipped, eye_coords, subject_id))\n",
    "    return corrupted_data\n",
    "\n",
    "# Evaluate the model with corrupted data\n",
    "def evaluate_with_corrupted_data(model, dataloader, gaze_model, noise_factor=0.1):\n",
    "    corrupted_data = create_corrupted_dataset(dataloader, noise_factor)\n",
    "    val_loss = 0.0\n",
    "    num_val_batches = 0\n",
    "\n",
    "    for i, (corrupted_face_image, corrupted_left_eye_image, corrupted_right_eye_image, rotation_matrix, rotation_matrix_flipped, gaze_2d, gaze_3d, gaze_3d_flipped, eye_coords, subject_id) in enumerate(corrupted_data):\n",
    "        input_dict = {\n",
    "            'face': corrupted_face_image,\n",
    "            'flipped_face': tf.image.flip_left_right(corrupted_face_image),\n",
    "            'lefteye': corrupted_left_eye_image,\n",
    "            'righteye': corrupted_right_eye_image,\n",
    "            'id': tf.constant([0 if s == 'p00' else 1 if s == 'p01' else 2 for s in subject_id], dtype=tf.int32),\n",
    "            'rotation_matrix': rotation_matrix,\n",
    "            'rotation_matrix_flipped': rotation_matrix_flipped,\n",
    "            'eye_coords': eye_coords,\n",
    "            'gaze': gaze_3d,\n",
    "            'gaze_flipped': gaze_3d_flipped\n",
    "        }\n",
    "        predicted_embeddings = model(input_dict)\n",
    "        \n",
    "        # Accuracy loss\n",
    "        preference_vectors = gaze_model.embedding(input_dict['id'])\n",
    "        accuracy_loss = tf.reduce_mean(tf.square(predicted_embeddings - preference_vectors))\n",
    "        \n",
    "        # Consistency loss\n",
    "        consistency_loss = embedding_consistency_loss(predicted_embeddings, batch_size=8)\n",
    "        \n",
    "        # Total loss\n",
    "        loss = accuracy_loss + consistency_loss\n",
    "        val_loss += loss.numpy()\n",
    "        num_val_batches += 1\n",
    "    \n",
    "    return val_loss / num_val_batches\n",
    "\n",
    "# Assess robustness with corrupted data\n",
    "corrupted_performance = evaluate_with_corrupted_data(calibration_model, val_dataloader, gaze_model, noise_factor=0.1)\n",
    "print(f\"Performance with Corrupted Data: {corrupted_performance}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  IMPLEMENTING THE SPAZE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-11T13:35:43.698957Z",
     "iopub.status.busy": "2024-07-11T13:35:43.698197Z",
     "iopub.status.idle": "2024-07-11T13:35:43.723476Z",
     "shell.execute_reply": "2024-07-11T13:35:43.722512Z",
     "shell.execute_reply.started": "2024-07-11T13:35:43.698920Z"
    }
   },
   "outputs": [],
   "source": [
    "#### UNCOMMENT THIS CELL IF YOU WANT TO TEST SPAZE MODEL ONLY. \n",
    "#### MAKE SURE YOU HAVE LOADED YOUR PRETRAINED GAZE AND CALIBRATION MODEL\n",
    "\n",
    "\n",
    "# import tensorflow as tf\n",
    "# from tensorflow.keras.layers import Flatten, Dense, BatchNormalization, Input\n",
    "# from tensorflow.keras.models import Model\n",
    "# from PIL import Image\n",
    "\n",
    "# # Define transformations using TensorFlow\n",
    "# def preprocess_image(image, target_size):\n",
    "#     image = tf.image.resize(image, target_size)\n",
    "#     image = tf.keras.applications.efficientnet_v2.preprocess_input(image)\n",
    "#     return image\n",
    "\n",
    "# class GazeDataset(tf.data.Dataset):\n",
    "#     def __new__(cls, subject_to_leave_out=None, batch_size=8, validation=False):\n",
    "#         def _generator():\n",
    "#             root_dir = 'processed_data'\n",
    "#             subjects = ['p00', 'p01', 'p02']\n",
    "#             transform_face = lambda img: preprocess_image(img, (224, 224))\n",
    "#             transform_eye = lambda img: preprocess_image(img, (112, 112))\n",
    "\n",
    "#             for subject in subjects:\n",
    "#                 if validation:\n",
    "#                     if subject != subject_to_leave_out:\n",
    "#                         continue\n",
    "#                 else:\n",
    "#                     if subject == subject_to_leave_out:\n",
    "#                         continue\n",
    "                \n",
    "#                 person_dir = os.path.join(root_dir, 'Image', subject)\n",
    "#                 for image_name in os.listdir(os.path.join(person_dir, 'face')):\n",
    "#                     face_image_path = os.path.join(person_dir, 'face', image_name)\n",
    "#                     left_eye_image_path = os.path.join(person_dir, 'lefteye', image_name)\n",
    "#                     right_eye_image_path = os.path.join(person_dir, 'righteye', image_name)\n",
    "#                     rotation_matrix_path = os.path.join(person_dir, 'rotation_matrix', image_name.replace('.jpg', '.npy'))\n",
    "#                     rotation_matrix_flipped_path = os.path.join(person_dir, 'rotation_matrix_flipped', image_name.replace('.jpg', '.npy'))\n",
    "#                     gaze_2d_path = os.path.join(person_dir, '2d_gaze', image_name.replace('.jpg', '.npy'))\n",
    "#                     gaze_3d_path = os.path.join(person_dir, '3d_gaze', image_name.replace('.jpg', '.npy'))\n",
    "#                     gaze_3d_flipped_path = os.path.join(person_dir, '3d_gaze_flipped', image_name.replace('.jpg', '.npy'))\n",
    "#                     eye_coords_path = os.path.join(person_dir, 'eye_coords', image_name.replace('.jpg', '.npy'))\n",
    "\n",
    "#                     face_image = Image.open(face_image_path).convert('RGB')\n",
    "#                     left_eye_image = Image.open(left_eye_image_path).convert('RGB')\n",
    "#                     right_eye_image = Image.open(right_eye_image_path).convert('RGB')\n",
    "\n",
    "#                     face_image = transform_face(np.array(face_image))\n",
    "#                     left_eye_image = transform_eye(np.array(left_eye_image))\n",
    "#                     right_eye_image = transform_eye(np.array(right_eye_image))\n",
    "\n",
    "#                     rotation_matrix = np.load(rotation_matrix_path)\n",
    "#                     rotation_matrix_flipped = np.load(rotation_matrix_flipped_path)\n",
    "#                     gaze_2d = np.load(gaze_2d_path)\n",
    "#                     gaze_3d = np.load(gaze_3d_path)\n",
    "#                     gaze_3d_flipped = np.load(gaze_3d_flipped_path)\n",
    "#                     eye_coords = np.load(eye_coords_path)\n",
    "\n",
    "#                     yield face_image, left_eye_image, right_eye_image, rotation_matrix, rotation_matrix_flipped, gaze_2d, gaze_3d, gaze_3d_flipped, eye_coords, subject\n",
    "\n",
    "#         return tf.data.Dataset.from_generator(\n",
    "#             _generator,\n",
    "#             output_signature=(\n",
    "#                 tf.TensorSpec(shape=(224, 224, 3), dtype=tf.float32),\n",
    "#                 tf.TensorSpec(shape=(112, 112, 3), dtype=tf.float32),\n",
    "#                 tf.TensorSpec(shape=(112, 112, 3), dtype=tf.float32),\n",
    "#                 tf.TensorSpec(shape=(3, 3), dtype=tf.float32),\n",
    "#                 tf.TensorSpec(shape=(3, 3), dtype=tf.float32),\n",
    "#                 tf.TensorSpec(shape=(2,), dtype=tf.float32),\n",
    "#                 tf.TensorSpec(shape=(3,), dtype=tf.float32),\n",
    "#                 tf.TensorSpec(shape=(3,), dtype=tf.float32),\n",
    "#                 tf.TensorSpec(shape=(6,), dtype=tf.float32),\n",
    "#                 tf.TensorSpec(shape=(), dtype=tf.string)\n",
    "#             )\n",
    "#         ).batch(batch_size)\n",
    "# # Define custom Huber loss function\n",
    "# def custom_huber_loss(y_true, y_pred, delta=1.5):\n",
    "#     error = y_true - y_pred\n",
    "#     is_small_error = tf.abs(error) <= delta\n",
    "\n",
    "#     small_error_loss = tf.square(error) / 2\n",
    "#     big_error_loss = delta * (tf.abs(error) - delta / 2)\n",
    "\n",
    "#     return tf.where(is_small_error, small_error_loss, big_error_loss)\n",
    "\n",
    "# # Calculate gaze loss as the average of Huber losses\n",
    "# def gaze_loss(y_true, y_pred, delta=1.5):\n",
    "#     huber_losses = custom_huber_loss(y_true, y_pred, delta)\n",
    "#     return tf.reduce_mean(huber_losses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-11T17:00:55.181335Z",
     "iopub.status.busy": "2024-07-11T17:00:55.180454Z",
     "iopub.status.idle": "2024-07-11T17:00:55.191230Z",
     "shell.execute_reply": "2024-07-11T17:00:55.190323Z",
     "shell.execute_reply.started": "2024-07-11T17:00:55.181305Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "\n",
    "# Define the SPAZE Model\n",
    "class SPAZEModel(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(SPAZEModel, self).__init__()\n",
    "        self.conv1 = Conv2D(64, (3, 3), activation='relu', padding='same')\n",
    "        self.pool1 = MaxPooling2D((2, 2))\n",
    "        self.conv2 = Conv2D(128, (3, 3), activation='relu', padding='same')\n",
    "        self.pool2 = MaxPooling2D((2, 2))\n",
    "        self.conv3 = Conv2D(256, (3, 3), activation='relu', padding='same')\n",
    "        self.pool3 = MaxPooling2D((2, 2))\n",
    "        self.flatten = Flatten()\n",
    "        self.fc1 = Dense(512, activation='relu')\n",
    "        self.dropout = Dropout(0.5)\n",
    "        self.fc2 = Dense(2, name='gaze_point')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.conv1(inputs)\n",
    "        x = self.pool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.pool2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.pool3(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-11T17:01:04.125436Z",
     "iopub.status.busy": "2024-07-11T17:01:04.124593Z",
     "iopub.status.idle": "2024-07-11T17:04:51.412959Z",
     "shell.execute_reply": "2024-07-11T17:04:51.412013Z",
     "shell.execute_reply.started": "2024-07-11T17:01:04.125405Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Leaving out subject: p00\n",
      "Epoch [1/2], Loss: 15.74140855112571\n",
      "Epoch [2/2], Loss: 0.00545260396009932\n",
      "Validation Loss on clean data for subject p00: 0.006276123268995434\n",
      "Validation Loss on corrupted data for subject p00: 0.0062777507392068705\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# this did training for spaze\n",
    "# Define the training function for SPAZE Model\n",
    "def train_spaze_model(model, dataloader, optimizer, num_epochs=2):\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        num_batches = 0\n",
    "        for i, (face_image, left_eye_image, right_eye_image, rotation_matrix, rotation_matrix_flipped, gaze_2d, gaze_3d, gaze_3d_flipped, eye_coords, subject_id) in enumerate(dataloader):\n",
    "            with tf.GradientTape() as tape:\n",
    "                outputs = model(face_image)\n",
    "                \n",
    "                # Debugging shape mismatch issue\n",
    "                #print(f\"gaze_2d shape: {gaze_2d.shape}\")\n",
    "                #print(f\"outputs shape: {outputs.shape}\")\n",
    "                \n",
    "                # Ensure gaze_2d and outputs have the same shape\n",
    "                if gaze_2d.shape != outputs.shape:\n",
    "                    gaze_2d = tf.reshape(gaze_2d, outputs.shape)\n",
    "                \n",
    "                loss = gaze_loss(gaze_2d, outputs)\n",
    "            grads = tape.gradient(loss, model.trainable_weights)\n",
    "            optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "            running_loss += loss.numpy()\n",
    "            num_batches += 1\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/num_batches}\")\n",
    "\n",
    "# Implementing the leave-one-out strategy for SPAZE model\n",
    "\n",
    "# TODO: Due to less compute resources , we considered only one subject for leave-out strategy\n",
    "subjects = ['p00']\n",
    "spaze_results = []\n",
    "\n",
    "for subject in subjects:\n",
    "    print(f\"Leaving out subject: {subject}\")\n",
    "    train_dataloader = GazeDataset(subject_to_leave_out=subject, batch_size=8)\n",
    "    val_dataloader = GazeDataset(subject_to_leave_out=subject, batch_size=8)\n",
    "    \n",
    "    # Initialize SPAZE model and optimizer\n",
    "    spaze_model = SPAZEModel()\n",
    "    spaze_optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "    \n",
    "    # Train the SPAZE model\n",
    "    train_spaze_model(spaze_model, train_dataloader, spaze_optimizer, num_epochs=2)\n",
    "    \n",
    "    # Evaluate the SPAZE model on clean data\n",
    "    val_loss_clean = 0.0\n",
    "    num_val_batches_clean = 0\n",
    "    for i, (face_image, left_eye_image, right_eye_image, rotation_matrix, rotation_matrix_flipped, gaze_2d, gaze_3d, gaze_3d_flipped, eye_coords, subject_id) in enumerate(val_dataloader):\n",
    "        outputs = spaze_model(face_image)\n",
    "        loss = gaze_loss(gaze_2d, outputs)\n",
    "        val_loss_clean += loss.numpy()\n",
    "        num_val_batches_clean += 1\n",
    "    avg_val_loss_clean = val_loss_clean / num_val_batches_clean\n",
    "    print(f\"Validation Loss on clean data for subject {subject}: {avg_val_loss_clean}\")\n",
    "    \n",
    "    # Evaluate the SPAZE model on corrupted data\n",
    "    val_loss_corrupted = 0.0\n",
    "    num_val_batches_corrupted = 0\n",
    "    for i, (face_image, left_eye_image, right_eye_image, rotation_matrix, rotation_matrix_flipped, gaze_2d, gaze_3d, gaze_3d_flipped, eye_coords, subject_id) in enumerate(val_dataloader):\n",
    "        corrupted_face_image = tf.image.random_brightness(face_image, max_delta=0.5)  # Example corruption\n",
    "        outputs = spaze_model(corrupted_face_image)\n",
    "        loss = gaze_loss(gaze_2d, outputs)\n",
    "        val_loss_corrupted += loss.numpy()\n",
    "        num_val_batches_corrupted += 1\n",
    "    avg_val_loss_corrupted = val_loss_corrupted / num_val_batches_corrupted\n",
    "    print(f\"Validation Loss on corrupted data for subject {subject}: {avg_val_loss_corrupted}\")\n",
    "    \n",
    "    # Collect results\n",
    "    results = {\n",
    "        'subject': subject,\n",
    "        'spaze_clean_loss': avg_val_loss_clean,\n",
    "        'spaze_corrupted_loss': avg_val_loss_corrupted\n",
    "    }\n",
    "    spaze_results.append(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-11T13:43:08.463217Z",
     "iopub.status.busy": "2024-07-11T13:43:08.462025Z",
     "iopub.status.idle": "2024-07-11T13:43:08.482475Z",
     "shell.execute_reply": "2024-07-11T13:43:08.481029Z",
     "shell.execute_reply.started": "2024-07-11T13:43:08.463181Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  subject  spaze_clean_loss  spaze_corrupted_loss\n",
      "0     p00          0.007282              0.007284\n"
     ]
    }
   ],
   "source": [
    "#import pandas as pd\n",
    "# Summarize and visualize SPAZE results\n",
    "#df_spaze_results = pd.DataFrame(spaze_results)\n",
    "#print(df_spaze_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMPARING PTGE AND SPAZE MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-11T17:38:14.725333Z",
     "iopub.status.busy": "2024-07-11T17:38:14.724935Z",
     "iopub.status.idle": "2024-07-11T18:12:56.809584Z",
     "shell.execute_reply": "2024-07-11T18:12:56.808634Z",
     "shell.execute_reply.started": "2024-07-11T17:38:14.725301Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating for subject: p00\n",
      "Results saved to 'ptge_spaze_comparison_results.csv'.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Define the evaluation function\n",
    "def evaluate_model(model, dataloader, corruption=None, model_type='ptge'):\n",
    "    total_loss = 0.0\n",
    "    num_batches = 0\n",
    "    results = []\n",
    "\n",
    "    for i, (face_image, left_eye_image, right_eye_image, rotation_matrix, rotation_matrix_flipped, gaze_2d, gaze_3d, gaze_3d_flipped, eye_coords, subject_id) in enumerate(dataloader):\n",
    "        if corruption == 'noise':\n",
    "            face_image += tf.random.normal(face_image.shape, mean=0.0, stddev=0.1)\n",
    "            left_eye_image += tf.random.normal(left_eye_image.shape, mean=0.0, stddev=0.1)\n",
    "            right_eye_image += tf.random.normal(right_eye_image.shape, mean=0.0, stddev=0.1)\n",
    "        elif corruption == 'blur':\n",
    "            face_image = tf.nn.conv2d(face_image, tf.random.normal((3, 3, 3, 3), mean=0.0, stddev=1.0), strides=[1, 1, 1, 1], padding='SAME')\n",
    "            left_eye_image = tf.nn.conv2d(left_eye_image, tf.random.normal((3, 3, 3, 3), mean=0.0, stddev=1.0), strides=[1, 1, 1, 1], padding='SAME')\n",
    "            right_eye_image = tf.nn.conv2d(right_eye_image, tf.random.normal((3, 3, 3, 3), mean=0.0, stddev=1.0), strides=[1, 1, 1, 1], padding='SAME')\n",
    "        # Add other types of corruption as needed\n",
    "\n",
    "        subject_indices = [int(s.decode().split('p')[1]) for s in subject_id.numpy()]\n",
    "\n",
    "        input_dict = {\n",
    "            'eye_coords': tf.convert_to_tensor(eye_coords, dtype=tf.float32),\n",
    "            'face': tf.convert_to_tensor(face_image, dtype=tf.float32),\n",
    "            'flipped_face': tf.convert_to_tensor(tf.image.flip_left_right(face_image), dtype=tf.float32),\n",
    "            'id': tf.convert_to_tensor(subject_indices, dtype=tf.int32),\n",
    "            'lefteye': tf.convert_to_tensor(left_eye_image, dtype=tf.float32),\n",
    "            'righteye': tf.convert_to_tensor(right_eye_image, dtype=tf.float32),\n",
    "            'rotation_matrix': tf.convert_to_tensor(rotation_matrix, dtype=tf.float32),\n",
    "            'rotation_matrix_flipped': tf.convert_to_tensor(rotation_matrix_flipped, dtype=tf.float32)\n",
    "        }\n",
    "\n",
    "        #print(f\"Batch {i} - Input Shapes and Types:\")\n",
    "        #for key, value in input_dict.items():\n",
    "         #   print(f\"{key}: shape={value.shape}, dtype={value.dtype}\")\n",
    "\n",
    "        if model_type == 'ptge':\n",
    "            # First, get the initial gaze estimation from the Gaze Model\n",
    "            initial_gaze_estimation = model['gaze_model'](input_dict)\n",
    "\n",
    "            #print(f\"Initial gaze estimation shape: {initial_gaze_estimation.shape}\")\n",
    "\n",
    "            # Now, use the Calibration Model to refine the gaze estimation\n",
    "            calibration_input_dict = input_dict.copy()\n",
    "            calibration_input_dict['gaze'] = initial_gaze_estimation\n",
    "            calibration_input_dict['gaze_flipped'] = initial_gaze_estimation  # No flipping, use as is\n",
    "\n",
    "            refined_gaze_estimation = model['calibration_model'](calibration_input_dict)\n",
    "\n",
    "            # Ensure the refined_gaze_estimation shape matches the gaze_3d shape\n",
    "            refined_gaze_estimation = refined_gaze_estimation[:, :3]  # Only take the first 3 columns\n",
    "\n",
    "            #print(f\"Refined gaze estimation shape: {refined_gaze_estimation.shape}\")\n",
    "\n",
    "            # Calculate loss\n",
    "            loss = gaze_loss(gaze_3d, refined_gaze_estimation)\n",
    "            results.append((gaze_3d.numpy(), refined_gaze_estimation.numpy()))\n",
    "\n",
    "        elif model_type == 'spaze':\n",
    "            # Get the gaze estimation from the SPAZE Model\n",
    "            spaze_gaze_estimation = model['spaze_model'](face_image, training=False)\n",
    "\n",
    "            #print(f\"SPAZE gaze estimation shape: {spaze_gaze_estimation.shape}\")\n",
    "\n",
    "            # Calculate loss\n",
    "            loss = gaze_loss(gaze_2d, spaze_gaze_estimation)\n",
    "            results.append((gaze_2d.numpy(), spaze_gaze_estimation.numpy()))\n",
    "\n",
    "        total_loss += loss.numpy()\n",
    "        num_batches += 1\n",
    "\n",
    "    average_loss = total_loss / num_batches\n",
    "    return average_loss, results\n",
    "\n",
    "# Load Pretrained Models in SavedModel format\n",
    "#gaze_model = tf.keras.models.load_model('gaze_model_1', compile=False)\n",
    "#calibration_model = tf.keras.models.load_model('calibration_model', compile=False)\n",
    "\n",
    "# Initialize the SPAZE model\n",
    "#spaze_model = SPAZEModel()\n",
    "\n",
    "# Prepare the dataframes for results\n",
    "results = {\n",
    "    'subject': [],\n",
    "    'ptge_clean_loss': [],\n",
    "    'ptge_corrupted_loss_noise': [],\n",
    "    'ptge_corrupted_loss_blur': [],\n",
    "    'spaze_clean_loss': [],\n",
    "    'spaze_corrupted_loss_noise': [],\n",
    "    'spaze_corrupted_loss_blur': [],\n",
    "}\n",
    "\n",
    "#subjects = ['p00', 'p01', 'p02']\n",
    "# TODO: Due to less compute resources , we considered only one subject for leave-out strategy\n",
    "subjects = ['p00']\n",
    "for subject in subjects:\n",
    "    print(f\"Evaluating for subject: {subject}\")\n",
    "    val_dataloader = GazeDataset(subject_to_leave_out=subject, batch_size=8)\n",
    "\n",
    "    # Evaluate PTGE model on clean data\n",
    "    ptge_clean_loss, _ = evaluate_model({'gaze_model': gaze_model, 'calibration_model': calibration_model}, val_dataloader, model_type='ptge')\n",
    "\n",
    "    # Evaluate PTGE model on corrupted data (noise)\n",
    "    ptge_corrupted_loss_noise, _ = evaluate_model({'gaze_model': gaze_model, 'calibration_model': calibration_model}, val_dataloader, corruption='noise', model_type='ptge')\n",
    "\n",
    "    # Evaluate PTGE model on corrupted data (blur)\n",
    "    ptge_corrupted_loss_blur, _ = evaluate_model({'gaze_model': gaze_model, 'calibration_model': calibration_model}, val_dataloader, corruption='blur', model_type='ptge')\n",
    "\n",
    "    # Evaluate SPAZE model on clean data\n",
    "    spaze_clean_loss, _ = evaluate_model({'spaze_model': spaze_model}, val_dataloader, model_type='spaze')\n",
    "\n",
    "    # Evaluate SPAZE model on corrupted data (noise)\n",
    "    spaze_corrupted_loss_noise, _ = evaluate_model({'spaze_model': spaze_model}, val_dataloader, corruption='noise', model_type='spaze')\n",
    "\n",
    "    # Evaluate SPAZE model on corrupted data (blur)\n",
    "    spaze_corrupted_loss_blur, _ = evaluate_model({'spaze_model': spaze_model}, val_dataloader, corruption='blur', model_type='spaze')\n",
    "\n",
    "    results['subject'].append(subject)\n",
    "    results['ptge_clean_loss'].append(ptge_clean_loss)\n",
    "    results['ptge_corrupted_loss_noise'].append(ptge_corrupted_loss_noise)\n",
    "    results['ptge_corrupted_loss_blur'].append(ptge_corrupted_loss_blur)\n",
    "    results['spaze_clean_loss'].append(spaze_clean_loss)\n",
    "    results['spaze_corrupted_loss_noise'].append(spaze_corrupted_loss_noise)\n",
    "    results['spaze_corrupted_loss_blur'].append(spaze_corrupted_loss_blur)\n",
    "\n",
    "# Convert the results to a DataFrame\n",
    "df_results = pd.DataFrame(results)\n",
    "df_results.to_csv('ptge_spaze_comparison_results.csv', index=False)\n",
    "print(\"Results saved to 'ptge_spaze_comparison_results.csv'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting the errors for PTGE and SPAZE models for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-11T18:36:33.828465Z",
     "iopub.status.busy": "2024-07-11T18:36:33.828066Z",
     "iopub.status.idle": "2024-07-11T18:36:33.832701Z",
     "shell.execute_reply": "2024-07-11T18:36:33.831725Z",
     "shell.execute_reply.started": "2024-07-11T18:36:33.828434Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-11T19:04:29.588628Z",
     "iopub.status.busy": "2024-07-11T19:04:29.587741Z",
     "iopub.status.idle": "2024-07-11T19:04:29.885953Z",
     "shell.execute_reply": "2024-07-11T19:04:29.885099Z",
     "shell.execute_reply.started": "2024-07-11T19:04:29.588592Z"
    }
   },
   "outputs": [],
   "source": [
    "# Convert to DataFrame\n",
    "df_results = pd.read_csv(\"/kaggle/working/ptge_spaze_comparison_results.csv\")\n",
    "\n",
    "#df_results = pd.read_csv(\"/kaggle/working/ptge_spaze_comparison_results.csv\")\n",
    "# Set the subject as the index (if not already set)\n",
    "if 'subject' in df_results.columns:\n",
    "    df_results.set_index('subject', inplace=True)\n",
    "\n",
    "# Plotting the results\n",
    "plt.figure(figsize=(14, 7))\n",
    "\n",
    "# Plot each model's losses\n",
    "plt.plot(df_results.index, df_results['ptge_clean_loss'], marker='o', label='PTGE Clean Loss', linestyle='--')\n",
    "plt.plot(df_results.index, df_results['ptge_corrupted_loss_noise'], marker='o', label='PTGE Corrupted Loss (Noise)', linestyle='--')\n",
    "plt.plot(df_results.index, df_results['ptge_corrupted_loss_blur'], marker='o', label='PTGE Corrupted Loss (Blur)', linestyle='--')\n",
    "\n",
    "plt.plot(df_results.index, df_results['spaze_clean_loss'], marker='o', label='SPAZE Clean Loss', linestyle='-')\n",
    "plt.plot(df_results.index, df_results['spaze_corrupted_loss_noise'], marker='o', label='SPAZE Corrupted Loss (Noise)', linestyle='-')\n",
    "plt.plot(df_results.index, df_results['spaze_corrupted_loss_blur'], marker='o', label='SPAZE Corrupted Loss (Blur)', linestyle='-')\n",
    "\n",
    "plt.title('Comparison of Losses for PTGE and SPAZE Models')\n",
    "plt.xlabel('Subject')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Save the plot to a file to ensure it's created\n",
    "plt.savefig('comparison_plot-ptge-spaze.png')\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 5345395,
     "sourceId": 8882920,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5348333,
     "sourceId": 8893813,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5349682,
     "sourceId": 8897885,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5349817,
     "sourceId": 8898290,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30733,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
